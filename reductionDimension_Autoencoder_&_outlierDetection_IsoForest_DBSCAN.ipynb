{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder_dimensionReduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPVMFGPYFs6F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "import queue\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTUFIujSEHQx"
      },
      "source": [
        "def loadAllData(path):\n",
        "    \"\"\"\n",
        "    Load all the volatility for all day in a numpy array\n",
        "    Use for the PCA\n",
        "    \"\"\"\n",
        "    #loading data (pickle file)\n",
        "    unpickled_df = pd.read_pickle(path)\n",
        "    Ndays = len(unpickled_df)\n",
        "    data = np.empty((Ndays,441))\n",
        "    j = 0\n",
        "    maturities = [5,13,36,58,80,110,140,200,300,400,500,800,900,1150,1400,1550,2000,2450,2700,2900,3200]\n",
        "    myrow = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
        "\n",
        "    for df in unpickled_df:\n",
        "        \n",
        "        df.set_index(['nBizDays'], inplace = True)\n",
        "            \n",
        "        \n",
        "        #Add rows in df for maturity that we want\n",
        "        for i in maturities:\n",
        "            exitingIndex = i in df.index\n",
        "            if exitingIndex == False :\n",
        "                \n",
        "                df.loc[i] = myrow\n",
        "        df.sort_index(inplace = True)\n",
        "\n",
        "        #interpolation\n",
        "        for col in df:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        \n",
        "        df.interpolate(method = \"values\", limit_direction = \"both\", inplace = True)\n",
        "        \n",
        "        #Fill allData in 1 np.array\n",
        "        df.drop(columns=['Forwards', 'nCalDays', 'diff Days'], inplace = True)\n",
        "        df = df.loc[maturities,:]\n",
        "        Nrows = len(df)\n",
        "        Ncolumns = len(df.columns)\n",
        "        data[j] = np.array(df.iloc[:,:], dtype = np.float).reshape(-1)\n",
        "        j = j + 1\n",
        "    return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkITY191E66a"
      },
      "source": [
        "data = loadAllData(\"NKY_clean.pkl\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpUuD3MAkmSr"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "n_train = int(len(data)*100/100)\n",
        "trainData = data[:n_train]\n",
        "testData = data[n_train:]\n",
        "mms = preprocessing.MinMaxScaler()\n",
        "trainDataScaled = mms.fit_transform(trainData)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL9DvpIqFo_r"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "encoding_dim = 12\n",
        "input = keras.Input(shape=(441,))\n",
        "encoded = layers.Dense(encoding_dim, activation = \"sigmoid\")(input)\n",
        "decoded = layers.Dense(441, activation = \"sigmoid\")(encoded)\n",
        "autoencoder = keras.Model(input,decoded)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-94499ba0304d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m441\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
            "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YcpVKr7HnX-"
      },
      "source": [
        "\n",
        "encoder = keras.Model(input, encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Rt1db3H2Wh"
      },
      "source": [
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "decoder_layer = autoencoder.layers[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "untzST_3Ii6M"
      },
      "source": [
        "decoder = keras.Model(encoded_input,decoder_layer(encoded_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQppsiyOIv3P"
      },
      "source": [
        "autoencoder.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8KEwDVOI9dK"
      },
      "source": [
        "model_autoencoder = autoencoder.fit(trainDataScaled, trainDataScaled,\n",
        "                epochs=500,\n",
        "                batch_size=100,\n",
        "                shuffle=True,\n",
        "                validation_split = 0.1,\n",
        "                verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "fqCAuy-mLfVr",
        "outputId": "e3903f76-4853-4dd5-80f0-eabd531285b8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model_autoencoder.history[\"loss\"], color = \"r\")\n",
        "plt.plot(model_autoencoder.history[\"val_loss\"], color = \"b\")\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn9JOxVdNOGi",
        "outputId": "539c7349-bc07-4852-ac03-8f7bd118fefd"
      },
      "source": [
        "import numpy as np\n",
        "v = np.random.normal(0,1,(1, 441))\n",
        "encoded_output = encoder.predict(trainDataScaled)\n",
        "decoded_output = decoder.predict(encoded_output)\n",
        "print(trainDataScaled[10][5:15])\n",
        "print(\"-------------------\")\n",
        "print(decoded_output[10][5:15])\n",
        "print(\"-------------------\")\n",
        "print(encoded_output)\n",
        "data = encoded_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5V4piyz-Cgv"
      },
      "source": [
        "class LeafNode:\n",
        "    def __init__(self, size, data):\n",
        "        self.size = size\n",
        "        self.data = data\n",
        "\n",
        "\n",
        "class DecisionNode:\n",
        "    def __init__(self, left, right, splitFeature, splitValue):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.splitFeature = splitFeature\n",
        "        self.splitValue = splitValue\n",
        "\n",
        "\n",
        "class IsolationTree:\n",
        "    def __init__(self, height, maxDepth):\n",
        "        self.height = height\n",
        "        self.maxDepth = maxDepth\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Given a 2D matrix of observations, create an isolation tree. Set field\n",
        "        self.root to the root of that tree and return it.\n",
        "        \"\"\"\n",
        "        if self.height >= self.maxDepth or X.shape[0] <= 2: #X.shapes[0] number of points\n",
        "            self.root = LeafNode(X.shape[0], X)\n",
        "            return self.root\n",
        "\n",
        "        # Choose Random Split features and Value\n",
        "        num_features = X.shape[1] #X.shapes[1] number of features\n",
        "        splitFeature = np.random.randint(0, num_features) #take radomly a feature\n",
        "        splitValue = np.random.uniform(min(X[:, splitFeature]), max(X[:, splitFeature])) #take randomly a value\n",
        "\n",
        "        X_left = X[X[:, splitFeature] < splitValue]\n",
        "        X_right = X[X[:, splitFeature] >= splitValue]\n",
        "\n",
        "        leftTree = IsolationTree(self.height + 1, self.maxDepth)\n",
        "        rightTree = IsolationTree(self.height + 1, self.maxDepth)\n",
        "        leftTree.fit(X_left)\n",
        "        rightTree.fit(X_right)\n",
        "        self.root = DecisionNode(leftTree.root, rightTree.root, splitFeature, splitValue)\n",
        "        self.n_nodes = self.count_nodes(self.root)\n",
        "        return self.root\n",
        "\n",
        "    def count_nodes(self, root):\n",
        "        count = 0\n",
        "        stack = [root]\n",
        "        while stack:\n",
        "            node = stack.pop()\n",
        "            count += 1\n",
        "            if isinstance(node, DecisionNode):\n",
        "                stack.append(node.right)\n",
        "                stack.append(node.left)\n",
        "        return count\n",
        "    \n",
        "class IsolationForest:\n",
        "    def __init__(self, sample_size, n_trees=10):\n",
        "        self.sample_size = sample_size\n",
        "        self.n_trees = n_trees\n",
        "\n",
        "    def fit(self, X): #X must be ndarray\n",
        "        \"\"\"\n",
        "        Given a 2D matrix of observations, create an ensemble of IsolationTree\n",
        "        objects and store them in a list: self.trees.  Convert DataFrames to\n",
        "        ndarray objects.\n",
        "        \"\"\"\n",
        "        self.trees = [] #array of n treess\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        n_rows = X.shape[0]\n",
        "        height_limit = np.ceil(np.log2(self.sample_size))\n",
        "        for i in range(self.n_trees):\n",
        "            data_index = np.random.choice(range(n_rows), size=self.sample_size, replace=False).astype(int)\n",
        "            #We are using the bootstrap in order to create new Sub_data\n",
        "            #choose randomly the sample (size = sample_size) from the dataSet wich we are going to apply isolation forest \n",
        "            #data_index = np.random.randint(0, n_rows, self.sample_size) \n",
        "            X_sub = X[data_index]\n",
        "            tree = IsolationTree(0, height_limit)\n",
        "            tree.fit(X_sub)\n",
        "            self.trees.append(tree)\n",
        "        return self\n",
        "\n",
        "    def path_length(self, X:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Given a 2D matrix of observations, X, compute the average path length\n",
        "        for each observation in X, we compute the path length for x_i using every\n",
        "        tree in self.trees then compute the average for each x_i.  \n",
        "        Return an ndarray of shape (len(X),1).\n",
        "        \"\"\"\n",
        "        paths = []\n",
        "        for row in X:\n",
        "            path = []\n",
        "            for tree in self.trees:\n",
        "                node = tree.root\n",
        "                length = 0\n",
        "                while isinstance(node, DecisionNode):\n",
        "                    if row[node.splitFeature] < node.splitValue:\n",
        "                        node = node.left\n",
        "                    else:\n",
        "                        node = node.right\n",
        "                    length += 1\n",
        "                leaf_size = node.size\n",
        "                pathLength = length + c(leaf_size)\n",
        "                path.append(pathLength)\n",
        "            paths.append(path)\n",
        "        paths = np.array(paths)\n",
        "        return np.mean(paths, axis=1)\n",
        "\n",
        "    def anomaly_score(self, X:pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Given a 2D matrix of observations, X, compute the anomaly score\n",
        "        for each x_i observation, returning an ndarray of them.\n",
        "        \"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        avg_length = self.path_length(X)\n",
        "        scores = np.array([np.power(2, -l/c(self.sample_size))for l in avg_length])\n",
        "        return scores\n",
        "\n",
        "    def predict_from_anomaly_scores(self, scores:np.ndarray, threshold:float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Given an array of scores and a score threshold, return an array of\n",
        "        the predictions: 1 for any score >= the threshold and 0 otherwise.\n",
        "        \"\"\"\n",
        "        return np.array([1 if s >= threshold else 0 for s in scores])\n",
        "\n",
        "    def predict(self, X:np.ndarray, threshold:float) -> np.ndarray:\n",
        "        \"A shorthand for calling anomaly_score() and predict_from_anomaly_scores().\"\n",
        "        scores = self.anomaly_score(X)\n",
        "        prediction = self.predict_from_anomaly_scores(scores, threshold)\n",
        "        return prediction    \n",
        "    \n",
        "def c(size):\n",
        "    if size > 2:\n",
        "        return 2 * (np.log(size-1)+0.5772156649) - 2*(size-1)/size\n",
        "    if size == 2:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def pltData(data, label):\n",
        "    fig = plt.figure()\n",
        "    #ax = fig.add_subplot(111, projection='3d')\n",
        "    l = [\"green\" if elt == 0 else \"red\" for elt in label] \n",
        "    #ax.scatter(data[:,0], data[:,1],data[:,2], c = label, s = 120)\n",
        "    plt.scatter(data[:,0], data[:,1], c = label, s = 120)\n",
        "    plt.xlabel(\"first dimension\")\n",
        "    plt.ylabel(\"second dimension\")\n",
        "    plt.show()\n",
        "\n",
        "def outliersDays(label):\n",
        "    outliers = []\n",
        "    for i in range(len(label)):\n",
        "        if label[i] == 1:\n",
        "            outliers.append(i)\n",
        "    return outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "hLl2igyzr6KN",
        "outputId": "da84c50b-a52f-4f20-d74d-1ed0001c8571"
      },
      "source": [
        "forest = IsolationForest(data.shape[0], 100)\n",
        "forest.fit(data)\n",
        "forest.path_length(data)\n",
        "prediction = forest.predict(data, 0.58)\n",
        "pltData(data, prediction)\n",
        "print(outliersDays(prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_cM06nPsie2"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import queue\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "class DBSCAN:\n",
        "    \n",
        "    def __init__(self, data, radius, MinPt):\n",
        "        self.data = data\n",
        "        self.radius = radius\n",
        "        self.MinPt = MinPt\n",
        "        self.noise = 0\n",
        "        self.unassigned = 0\n",
        "        self.core=-1\n",
        "        self.edge=-2\n",
        "        \n",
        "    #function to find all neigbor points in radius\n",
        "    def neighbor_points(self, pointId):\n",
        "        points = []\n",
        "        for i in range(len(self.data)):\n",
        "            #Euclidian distance using L2 Norm\n",
        "            if sum((self.data[i] - self.data[pointId])**2) <= self.radius**2:\n",
        "                points.append(i)\n",
        "        return points\n",
        "   \n",
        "    \n",
        "    def dbscan(self):\n",
        "        #initilize all pointlabel to unassign\n",
        "        pointlabel  = [self.unassigned] * len(self.data)\n",
        "        pointcount = []\n",
        "        #initialize list for core/noncore point\n",
        "        corepoint=[]\n",
        "        noncore=[]    \n",
        "        \n",
        "        #Find all neigbor for all point\n",
        "        for i in range(len(self.data)):\n",
        "            pointcount.append(DBSCAN.neighbor_points(self, i))\n",
        "    \n",
        "        #Find all core point, edgepoint and noise\n",
        "        for i in range(len(pointcount)):\n",
        "            if (len(pointcount[i])>=self.MinPt):\n",
        "                pointlabel[i]=self.core\n",
        "                corepoint.append(i)\n",
        "            else:\n",
        "                noncore.append(i)    \n",
        "                \n",
        "        for i in noncore:\n",
        "            for j in pointcount[i]:\n",
        "                if j in corepoint:\n",
        "                    pointlabel[i]=self.edge\n",
        "                    break\n",
        "                \n",
        "        #start assigning point to luster\n",
        "        cl = 1\n",
        "        #Using a Queue to put all neigbor core point in queue and find neigboir's neigbor\n",
        "        for i in range(len(pointlabel)):\n",
        "            q = queue.Queue()\n",
        "            if (pointlabel[i] == self.core):\n",
        "                pointlabel[i] = cl\n",
        "                for x in pointcount[i]:\n",
        "                    if(pointlabel[x]==self.core):\n",
        "                        q.put(x)\n",
        "                        pointlabel[x]=cl\n",
        "                    elif(pointlabel[x]==self.edge):\n",
        "                        pointlabel[x]=cl\n",
        "                #Stop when all point in Queue has been checked   \n",
        "                while not q.empty():\n",
        "                    neighbors = pointcount[q.get()]\n",
        "                    for y in neighbors:\n",
        "                        if (pointlabel[y]==self.core):\n",
        "                            pointlabel[y]=cl\n",
        "                            q.put(y)\n",
        "                        if (pointlabel[y]==self.edge):\n",
        "                            pointlabel[y]=cl            \n",
        "                cl=cl+1 #move to next cluster\n",
        "                \n",
        "        return pointlabel,cl\n",
        "    \n",
        "    #Function to plot final result with different clusters and anomalies\n",
        "    def plotRes(self, clusterRes, clusterNum):\n",
        "        nPoints = len(self.data)\n",
        "        scatterColors = ['black', 'green', 'brown', 'red', 'purple', 'orange', 'yellow']\n",
        "        for i in range(clusterNum):\n",
        "            if (i==0):\n",
        "                #Plot all noise point as blue\n",
        "                color='blue'\n",
        "            else:\n",
        "                color = scatterColors[i % len(scatterColors)]\n",
        "            abscissa = []\n",
        "            ordinate = []\n",
        "            for j in range(nPoints):\n",
        "                if clusterRes[j] == i:\n",
        "                    abscissa.append(self.data[j, 0])\n",
        "                    ordinate.append(self.data[j, 1])\n",
        "            plt.scatter(abscissa, ordinate, c=color, alpha=1, marker='.', s = 120)\n",
        "    \n",
        "    def plotRes3D(self, clusterNum):\n",
        "        #plot\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        scatterColors = ['black', 'green', 'brown', 'red', 'purple', 'orange', 'yellow']\n",
        "        for i in range(clusterNum):\n",
        "            if (i==0):\n",
        "                #Plot all noise point as blue\n",
        "                color='red'\n",
        "            else:\n",
        "                color = scatterColors[i % len(scatterColors)]\n",
        "            ax.scatter(self.data[:,0], self.data[:,1], self.data[:,2], c = color)\n",
        "        plt.show()\n",
        "\n",
        "def days_outliers(data):\n",
        "        res = []\n",
        "        for i in range(len(data)):\n",
        "            if data[i]==0:\n",
        "                res.append(i)\n",
        "        return res\n",
        "\n",
        "def Distances(data):\n",
        "    distance=[]\n",
        "    for i in range(len(data)):\n",
        "        distance1=0\n",
        "        for j in range(len(data)):\n",
        "            if i!= j:\n",
        "                distance1 += np.linalg.norm(data[i] - data[j])\n",
        "        distance.append(distance)\n",
        "    return distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "wcOefSmi8JYF",
        "outputId": "0e4f00cc-0500-4144-b688-c8a050f57e8f"
      },
      "source": [
        "k = 10\n",
        "NN = NearestNeighbors(n_neighbors = k).fit(data)\n",
        "distances, indices = NN.kneighbors(data)\n",
        "distanceSorted = sorted(distances[:,k-1], reverse = True)\n",
        "radiusArray = np.percentile(distanceSorted,99)\n",
        "clustering = DBSCAN(data, radiusArray, 20)\n",
        "print('Set radius = ' +str(clustering.radius)+ ', Minpoints = '+str(clustering.MinPt))\n",
        "pointlabel, cluster = clustering.dbscan()\n",
        "clustering.plotRes3D(pointlabel, cluster)\n",
        "plt.show()\n",
        "print('Number of clusters found: ' + str(cluster - 1))\n",
        "counter=collections.Counter(pointlabel)\n",
        "print(counter)\n",
        "outliers  = pointlabel.count(0)\n",
        "print('Numbrer of outliers found: '+str(outliers) +'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqPjTqng8WZP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}